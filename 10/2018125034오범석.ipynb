{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17cde0c3",
   "metadata": {},
   "source": [
    "# 오늘은 LeNet 구조를 만들어봅시다\n",
    "\n",
    "\n",
    "LeNet 구조는 CNN이며, 초기에 만들어진 모델입니다. \n",
    "\n",
    "2가지 모델(Sigmoid, ReLU)를 만들어 두 모델의 성능을 비교해봅시다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aff3fd",
   "metadata": {},
   "source": [
    "## 1.우선 필요 라이브러리를 import 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd17ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798bac6b",
   "metadata": {},
   "source": [
    "## 2. 딥러닝 모델을 설계할 때 활용하는 장비 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c9880ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.13.1  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5590af",
   "metadata": {},
   "source": [
    "## 3. MNIST 데이터 다운로드 \n",
    "\n",
    " 1. Training data와 Test data 분리하기\n",
    " \n",
    " 2. Training data를 Training data 와 Validation data로 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00908077",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_data = datasets.MNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = datasets.MNIST('./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "mean = train_data.data.float().mean(axis=(0,1,2))/255\n",
    "std = train_data.data.float().std(axis=(0,1,2))/255\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean,std)\n",
    "    ])\n",
    "\n",
    "train, val = torch.utils.data.random_split(train_data, [int(len(train_data) * 0.8),int(len(train_data) * 0.2)])\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437ffb80",
   "metadata": {},
   "source": [
    "## 4. torch.nn을 이용하여 모델-1 만들기\n",
    "\n",
    "   1) 아래의 그림 중 LeNet 구조를 구현 할 것\n",
    "   \n",
    "   2) Sigmoid 활성화 함수를 이용할 것\n",
    "   \n",
    "   \n",
    "![](Comparison_image_neural_networks.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "defacffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Model_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5,stride=1,padding=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5,stride=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=120,kernel_size=5,stride=1)\n",
    "        self.fc1 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc2 = nn.Linear(in_features=84, out_features=10)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.avg_pool = nn.AvgPool2d(2,stride=2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.sigmoid(x) # 활성함수\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.conv2(x)   \n",
    "        x = self.sigmoid(x) # 활성함수\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(-1,120) # flatten\n",
    "        x = self.fc1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26eed9",
   "metadata": {},
   "source": [
    "## 5. torch.nn을 이용하여 모델-2 만들기\n",
    "\n",
    "   LeNet 모델에서 ReLU 활성화 함수를 사용하시요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27ac70af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=120,kernel_size=5)\n",
    "        self.fc1 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc2 = nn.Linear(in_features=84, out_features=10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.avg_pool = nn.AvgPool2d(2,stride=2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x) # 활성함수\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x) # 활성함수\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(-1,120) # flatten\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de556825",
   "metadata": {},
   "source": [
    "## 7. 학습 준비하기\n",
    "\n",
    "1) 1 epoch를 학습할 수 있는 함수 만들기\n",
    "\n",
    "2) Test와 Validation data의 정확도 계산할 수 있는 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06030b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_epoch(train_loader, network, loss_func, optimizer, epoch):\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    log_interval = 300\n",
    "    \n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image, label = image.to(device), label.to(device)\n",
    "\n",
    "        # 미분값의 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward propagration 계산하기.\n",
    "        outputs = network(image)\n",
    "        \n",
    "        # Cross_entropy 함수를 적용하여 loss를 구하고 저장하기\n",
    "        loss = loss_func(outputs,label)\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # training accuracy 정확도 구하기 위해 맞는 샘플 개수 세기\n",
    "        pred = outputs.data.max(1)[1]\n",
    "        train_correct += pred.eq(label).sum()\n",
    "\n",
    "        # Gradinet 구하기\n",
    "        loss.backward()\n",
    "\n",
    "        # weight값 update 하기\n",
    "        optimizer.step()\n",
    "\n",
    "        # 학습 상황 출력\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.2f}%)]\\tLoss: {:.6f}'\n",
    "                  .format(epoch, batch_idx * len(label), len(train_loader.dataset),100. * batch_idx / len(train_loader),\n",
    "                          loss.item()))\n",
    "            \n",
    "    return train_losses, train_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3c0dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(test_loader, network, loss_func, val = False):\n",
    "    correct = 0\n",
    "    \n",
    "    test_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (image, label) in enumerate(test_loader):\n",
    "            image, label = image.to(device), label.to(device)\n",
    "\n",
    "            # Forward propagration 계산하기.\n",
    "            outputs = network(image)\n",
    "\n",
    "            # Cross_entropy 함수를 적용하여 loss를 구하기\n",
    "            loss = loss_func(outputs,label)\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "            # Batch 별로 정확도 구하기\n",
    "            pred = outputs.data.max(1)[1]\n",
    "            correct += pred.eq(label).sum()\n",
    "\n",
    "        # 전체 정확도 구하기\n",
    "        test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "        #중간결과 출력\n",
    "        if val is True:\n",
    "                print('Validation set: Accuracy: {}/{} ({:.2f}%)\\n'\n",
    "              .format(correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n",
    "        \n",
    "        else:\n",
    "            print('Test set: Accuracy: {}/{} ({:.2f}%)\\n'\n",
    "                  .format(correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n",
    "        \n",
    "    return test_losses, test_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d73c53",
   "metadata": {},
   "source": [
    "## 8. 위 정의된 함수로 학습 함수 만들기\n",
    "\n",
    "Adam Optimizer를 사용하여 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df29783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(network, learning_rate = 0.001):\n",
    "    \n",
    "    epoches = 15\n",
    "    \n",
    "    cls_loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_losses_per_epoch = []\n",
    "    test_losses_per_epoch = []\n",
    "    \n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    \n",
    "    for epoch in range(epoches):\n",
    "                \n",
    "        # 모델를 학습 중이라고 선언하기\n",
    "        network.train()\n",
    "        \n",
    "        train_losses, train_correct = training_epoch(train_loader,network,cls_loss,optimizer, epoch)\n",
    "        \n",
    "        # epoch 별로 loss 평균값, 정확도 구하기\n",
    "        average_loss = np.mean(train_losses)\n",
    "        train_losses_per_epoch.append(average_loss)\n",
    "        \n",
    "        train_accuracy = train_correct / len(train_loader.dataset) * 100\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # epoch 별로 정확도 출력\n",
    "        print('\\nTraining set: Accuracy: {}/{} ({:.2f}%)'\n",
    "              .format(train_correct, len(train_loader.dataset),100. * train_correct / len(train_loader.dataset)))\n",
    "\n",
    "        \n",
    "        ### 학습 중에 test 결과 보기\n",
    "        \n",
    "        # 모델 test 중인 것을 선언하기\n",
    "        network.eval()\n",
    "        \n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            test_losses, test_accuracy = test_epoch(val_loader, network, cls_loss, True)\n",
    "\n",
    "        test_losses_per_epoch.append(np.mean(test_losses))\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        test_losses, test_accuracy = test_epoch(test_loader, network, cls_loss, False)\n",
    "        \n",
    "    return train_losses_per_epoch, test_losses_per_epoch, train_accuracies, test_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1394321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_1(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc2): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (avg_pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 28, 28]             156\n",
      "           Sigmoid-2            [-1, 6, 28, 28]               0\n",
      "         AvgPool2d-3            [-1, 6, 14, 14]               0\n",
      "            Conv2d-4           [-1, 16, 10, 10]           2,416\n",
      "           Sigmoid-5           [-1, 16, 10, 10]               0\n",
      "         AvgPool2d-6             [-1, 16, 5, 5]               0\n",
      "            Conv2d-7            [-1, 120, 1, 1]          48,120\n",
      "            Linear-8                   [-1, 84]          10,164\n",
      "           Sigmoid-9                   [-1, 84]               0\n",
      "           Linear-10                   [-1, 10]             850\n",
      "          Sigmoid-11                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.11\n",
      "Params size (MB): 0.24\n",
      "Estimated Total Size (MB): 0.35\n",
      "----------------------------------------------------------------\n",
      "2018125034\n",
      "Train Epoch: 0 [0/48000 (0.00%)]\tLoss: 2.301453\n",
      "Train Epoch: 0 [19200/48000 (40.00%)]\tLoss: 1.929557\n",
      "Train Epoch: 0 [38400/48000 (80.00%)]\tLoss: 1.701367\n",
      "\n",
      "Training set: Accuracy: 28008/48000 (58.35%)\n",
      "Validation set: Accuracy: 10318/12000 (85.98%)\n",
      "\n",
      "Train Epoch: 1 [0/48000 (0.00%)]\tLoss: 1.616702\n",
      "Train Epoch: 1 [19200/48000 (40.00%)]\tLoss: 1.645492\n",
      "Train Epoch: 1 [38400/48000 (80.00%)]\tLoss: 1.604506\n",
      "\n",
      "Training set: Accuracy: 42019/48000 (87.54%)\n",
      "Validation set: Accuracy: 10698/12000 (89.15%)\n",
      "\n",
      "Train Epoch: 2 [0/48000 (0.00%)]\tLoss: 1.549091\n",
      "Train Epoch: 2 [19200/48000 (40.00%)]\tLoss: 1.593053\n",
      "Train Epoch: 2 [38400/48000 (80.00%)]\tLoss: 1.565741\n",
      "\n",
      "Training set: Accuracy: 42911/48000 (89.40%)\n",
      "Validation set: Accuracy: 10842/12000 (90.35%)\n",
      "\n",
      "Train Epoch: 3 [0/48000 (0.00%)]\tLoss: 1.520317\n",
      "Train Epoch: 3 [19200/48000 (40.00%)]\tLoss: 1.555095\n",
      "Train Epoch: 3 [38400/48000 (80.00%)]\tLoss: 1.533097\n",
      "\n",
      "Training set: Accuracy: 44101/48000 (91.88%)\n",
      "Validation set: Accuracy: 11289/12000 (94.07%)\n",
      "\n",
      "Train Epoch: 4 [0/48000 (0.00%)]\tLoss: 1.505927\n",
      "Train Epoch: 4 [19200/48000 (40.00%)]\tLoss: 1.526738\n",
      "Train Epoch: 4 [38400/48000 (80.00%)]\tLoss: 1.511303\n",
      "\n",
      "Training set: Accuracy: 45852/48000 (95.53%)\n",
      "Validation set: Accuracy: 11549/12000 (96.24%)\n",
      "\n",
      "Train Epoch: 5 [0/48000 (0.00%)]\tLoss: 1.486029\n",
      "Train Epoch: 5 [19200/48000 (40.00%)]\tLoss: 1.510566\n",
      "Train Epoch: 5 [38400/48000 (80.00%)]\tLoss: 1.499893\n",
      "\n",
      "Training set: Accuracy: 46356/48000 (96.57%)\n",
      "Validation set: Accuracy: 11621/12000 (96.84%)\n",
      "\n",
      "Train Epoch: 6 [0/48000 (0.00%)]\tLoss: 1.480762\n",
      "Train Epoch: 6 [19200/48000 (40.00%)]\tLoss: 1.501481\n",
      "Train Epoch: 6 [38400/48000 (80.00%)]\tLoss: 1.492860\n",
      "\n",
      "Training set: Accuracy: 46597/48000 (97.08%)\n",
      "Validation set: Accuracy: 11654/12000 (97.12%)\n",
      "\n",
      "Train Epoch: 7 [0/48000 (0.00%)]\tLoss: 1.475626\n",
      "Train Epoch: 7 [19200/48000 (40.00%)]\tLoss: 1.487063\n",
      "Train Epoch: 7 [38400/48000 (80.00%)]\tLoss: 1.489018\n",
      "\n",
      "Training set: Accuracy: 46768/48000 (97.43%)\n",
      "Validation set: Accuracy: 11683/12000 (97.36%)\n",
      "\n",
      "Train Epoch: 8 [0/48000 (0.00%)]\tLoss: 1.472312\n",
      "Train Epoch: 8 [19200/48000 (40.00%)]\tLoss: 1.481480\n",
      "Train Epoch: 8 [38400/48000 (80.00%)]\tLoss: 1.487240\n",
      "\n",
      "Training set: Accuracy: 46877/48000 (97.66%)\n",
      "Validation set: Accuracy: 11718/12000 (97.65%)\n",
      "\n",
      "Train Epoch: 9 [0/48000 (0.00%)]\tLoss: 1.469206\n",
      "Train Epoch: 9 [19200/48000 (40.00%)]\tLoss: 1.478015\n",
      "Train Epoch: 9 [38400/48000 (80.00%)]\tLoss: 1.483603\n",
      "\n",
      "Training set: Accuracy: 46964/48000 (97.84%)\n",
      "Validation set: Accuracy: 11738/12000 (97.82%)\n",
      "\n",
      "Train Epoch: 10 [0/48000 (0.00%)]\tLoss: 1.467241\n",
      "Train Epoch: 10 [19200/48000 (40.00%)]\tLoss: 1.474492\n",
      "Train Epoch: 10 [38400/48000 (80.00%)]\tLoss: 1.480915\n",
      "\n",
      "Training set: Accuracy: 47034/48000 (97.99%)\n",
      "Validation set: Accuracy: 11758/12000 (97.98%)\n",
      "\n",
      "Train Epoch: 11 [0/48000 (0.00%)]\tLoss: 1.465504\n",
      "Train Epoch: 11 [19200/48000 (40.00%)]\tLoss: 1.474622\n",
      "Train Epoch: 11 [38400/48000 (80.00%)]\tLoss: 1.473045\n",
      "\n",
      "Training set: Accuracy: 47085/48000 (98.09%)\n",
      "Validation set: Accuracy: 11760/12000 (98.00%)\n",
      "\n",
      "Train Epoch: 12 [0/48000 (0.00%)]\tLoss: 1.464885\n",
      "Train Epoch: 12 [19200/48000 (40.00%)]\tLoss: 1.473305\n",
      "Train Epoch: 12 [38400/48000 (80.00%)]\tLoss: 1.475929\n",
      "\n",
      "Training set: Accuracy: 47128/48000 (98.18%)\n",
      "Validation set: Accuracy: 11763/12000 (98.03%)\n",
      "\n",
      "Train Epoch: 13 [0/48000 (0.00%)]\tLoss: 1.463825\n",
      "Train Epoch: 13 [19200/48000 (40.00%)]\tLoss: 1.473360\n",
      "Train Epoch: 13 [38400/48000 (80.00%)]\tLoss: 1.473588\n",
      "\n",
      "Training set: Accuracy: 47194/48000 (98.32%)\n",
      "Validation set: Accuracy: 11786/12000 (98.22%)\n",
      "\n",
      "Train Epoch: 14 [0/48000 (0.00%)]\tLoss: 1.464838\n",
      "Train Epoch: 14 [19200/48000 (40.00%)]\tLoss: 1.471856\n",
      "Train Epoch: 14 [38400/48000 (80.00%)]\tLoss: 1.470762\n",
      "\n",
      "Training set: Accuracy: 47230/48000 (98.40%)\n",
      "Validation set: Accuracy: 11782/12000 (98.18%)\n",
      "\n",
      "Test set: Accuracy: 9853/10000 (98.53%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network = Model_1().to(device)\n",
    "print(network)\n",
    "summary(network, input_size=(1, 28, 28))\n",
    "print(2018125034)\n",
    "rlt_const = training(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64815daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018125034\n",
      "Train Epoch: 0 [0/48000 (0.00%)]\tLoss: 2.306522\n",
      "Train Epoch: 0 [19200/48000 (40.00%)]\tLoss: 1.288931\n",
      "Train Epoch: 0 [38400/48000 (80.00%)]\tLoss: 0.945398\n",
      "\n",
      "Training set: Accuracy: 26817/48000 (55.87%)\n",
      "Validation set: Accuracy: 7001/12000 (58.34%)\n",
      "\n",
      "Train Epoch: 1 [0/48000 (0.00%)]\tLoss: 1.116936\n",
      "Train Epoch: 1 [19200/48000 (40.00%)]\tLoss: 1.109284\n",
      "Train Epoch: 1 [38400/48000 (80.00%)]\tLoss: 0.775662\n",
      "\n",
      "Training set: Accuracy: 28548/48000 (59.47%)\n",
      "Validation set: Accuracy: 7141/12000 (59.51%)\n",
      "\n",
      "Train Epoch: 2 [0/48000 (0.00%)]\tLoss: 1.041174\n",
      "Train Epoch: 2 [19200/48000 (40.00%)]\tLoss: 1.065816\n",
      "Train Epoch: 2 [38400/48000 (80.00%)]\tLoss: 0.670521\n",
      "\n",
      "Training set: Accuracy: 28868/48000 (60.14%)\n",
      "Validation set: Accuracy: 7198/12000 (59.98%)\n",
      "\n",
      "Train Epoch: 3 [0/48000 (0.00%)]\tLoss: 1.023185\n",
      "Train Epoch: 3 [19200/48000 (40.00%)]\tLoss: 0.739097\n",
      "Train Epoch: 3 [38400/48000 (80.00%)]\tLoss: 0.504236\n",
      "\n",
      "Training set: Accuracy: 32806/48000 (68.35%)\n",
      "Validation set: Accuracy: 8388/12000 (69.90%)\n",
      "\n",
      "Train Epoch: 4 [0/48000 (0.00%)]\tLoss: 0.731572\n",
      "Train Epoch: 4 [19200/48000 (40.00%)]\tLoss: 0.663469\n",
      "Train Epoch: 4 [38400/48000 (80.00%)]\tLoss: 0.482191\n",
      "\n",
      "Training set: Accuracy: 33789/48000 (70.39%)\n",
      "Validation set: Accuracy: 8396/12000 (69.97%)\n",
      "\n",
      "Train Epoch: 5 [0/48000 (0.00%)]\tLoss: 0.737265\n",
      "Train Epoch: 5 [19200/48000 (40.00%)]\tLoss: 0.657817\n",
      "Train Epoch: 5 [38400/48000 (80.00%)]\tLoss: 0.482501\n",
      "\n",
      "Training set: Accuracy: 33876/48000 (70.57%)\n",
      "Validation set: Accuracy: 8392/12000 (69.93%)\n",
      "\n",
      "Train Epoch: 6 [0/48000 (0.00%)]\tLoss: 0.740336\n",
      "Train Epoch: 6 [19200/48000 (40.00%)]\tLoss: 0.653064\n",
      "Train Epoch: 6 [38400/48000 (80.00%)]\tLoss: 0.476438\n",
      "\n",
      "Training set: Accuracy: 33941/48000 (70.71%)\n",
      "Validation set: Accuracy: 8401/12000 (70.01%)\n",
      "\n",
      "Train Epoch: 7 [0/48000 (0.00%)]\tLoss: 0.733749\n",
      "Train Epoch: 7 [19200/48000 (40.00%)]\tLoss: 0.653058\n",
      "Train Epoch: 7 [38400/48000 (80.00%)]\tLoss: 0.461455\n",
      "\n",
      "Training set: Accuracy: 33973/48000 (70.78%)\n",
      "Validation set: Accuracy: 8393/12000 (69.94%)\n",
      "\n",
      "Train Epoch: 8 [0/48000 (0.00%)]\tLoss: 0.726788\n",
      "Train Epoch: 8 [19200/48000 (40.00%)]\tLoss: 0.656038\n",
      "Train Epoch: 8 [38400/48000 (80.00%)]\tLoss: 0.435970\n",
      "\n",
      "Training set: Accuracy: 33988/48000 (70.81%)\n",
      "Validation set: Accuracy: 8423/12000 (70.19%)\n",
      "\n",
      "Train Epoch: 9 [0/48000 (0.00%)]\tLoss: 0.726339\n",
      "Train Epoch: 9 [19200/48000 (40.00%)]\tLoss: 0.654608\n",
      "Train Epoch: 9 [38400/48000 (80.00%)]\tLoss: 0.435398\n",
      "\n",
      "Training set: Accuracy: 34021/48000 (70.88%)\n",
      "Validation set: Accuracy: 8419/12000 (70.16%)\n",
      "\n",
      "Train Epoch: 10 [0/48000 (0.00%)]\tLoss: 0.722626\n",
      "Train Epoch: 10 [19200/48000 (40.00%)]\tLoss: 0.654112\n",
      "Train Epoch: 10 [38400/48000 (80.00%)]\tLoss: 0.442856\n",
      "\n",
      "Training set: Accuracy: 34068/48000 (70.97%)\n",
      "Validation set: Accuracy: 8414/12000 (70.12%)\n",
      "\n",
      "Train Epoch: 11 [0/48000 (0.00%)]\tLoss: 0.727767\n",
      "Train Epoch: 11 [19200/48000 (40.00%)]\tLoss: 0.654280\n",
      "Train Epoch: 11 [38400/48000 (80.00%)]\tLoss: 0.434362\n",
      "\n",
      "Training set: Accuracy: 34068/48000 (70.97%)\n",
      "Validation set: Accuracy: 8432/12000 (70.27%)\n",
      "\n",
      "Train Epoch: 12 [0/48000 (0.00%)]\tLoss: 0.722616\n",
      "Train Epoch: 12 [19200/48000 (40.00%)]\tLoss: 0.659238\n",
      "Train Epoch: 12 [38400/48000 (80.00%)]\tLoss: 0.432302\n",
      "\n",
      "Training set: Accuracy: 34099/48000 (71.04%)\n",
      "Validation set: Accuracy: 8417/12000 (70.14%)\n",
      "\n",
      "Train Epoch: 13 [0/48000 (0.00%)]\tLoss: 0.722997\n",
      "Train Epoch: 13 [19200/48000 (40.00%)]\tLoss: 0.653009\n"
     ]
    }
   ],
   "source": [
    "network = Model_2().to(device)\n",
    "print(2018125034)\n",
    "rlt_const = training(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791b1471",
   "metadata": {},
   "source": [
    "## 9. 두모델의 성능을 비교하시오"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f8156f",
   "metadata": {},
   "source": [
    "정답) 활성화 함수를 ReLu가 아닌, Sigmoid를 사용했을 때 더 성능이 높게 나왔다. <br>\n",
    "그 이유는, MNIST 데이터는 확률에 기반하여 예측하는데 시그모이드 함수는 0에서 1사이 값 중, 레이블과 일치할 가능성이 가장 높은 숫자를 선택하는 반면,\n",
    "ReLU함수는 0 또는 x 값 자체를 가지기 때문인듯 하다.<br>\n",
    "만약 입력이미지의 범위를 [0,255]에서 [0,1]로 변환한다면 ReLU도 더 높은 정확도를 가질 수 있지 않을까 추측해본다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
